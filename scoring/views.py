import asyncio
import hashlib
import json
import os
import re
import uuid
import pandas as pd

from datetime import datetime
from collections import defaultdict, OrderedDict
from django.http import JsonResponse, HttpRequest, FileResponse, Http404, StreamingHttpResponse
from django.shortcuts import render
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search, Q
from langchain.vectorstores.milvus import Milvus
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from pymilvus import MilvusClient
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, BertModel
from FlagEmbedding import FlagReranker
from concurrent.futures import ThreadPoolExecutor

from ai_llm_competition.settings import es_config, milvus_config, conn, BASE_DIR, get_Qianfan, get_Tongyi, get_Spark, \
    llm_flow, generate_text

os.chdir('./')

# ----------------------------------------------- çŸ¥è¯†å¬å›åŠŸèƒ½ -----------------------------------------------------------
embeddings = {}
_bert_cache = {}
_bert_tokenizer = None
_bert_model = None
milvus_retriever = None

es_client = Elasticsearch(es_config['host'], basic_auth=es_config['basic_auth'])


def clean_text(text):
    # å»é™¤ç‰¹å®šå­—ç¬¦å’Œè¿ç»­çš„æ˜Ÿå·
    text = re.sub(r'[^\w\sï¼Œã€‚ã€ï¼š#ã€ã€‘ï¼ˆï¼‰,.-]', '', text, flags=re.IGNORECASE)
    # æ›¿æ¢é‡‘é¢å’Œè¿ç»­çš„æ˜Ÿå·
    pattern = r'\d{1,3}(?:,\d{3})*\.\d+|\d{1,3}(?:,\d{3})*|\d+'
    text = re.sub(pattern, lambda match: '*', text)
    text = re.sub(r'xx+', '*', text)
    text = re.sub(r'\*+', '*', text)
    return text.strip()


def generate_unique_identifier(text):
    # ä½¿ç”¨ SHA-256 å“ˆå¸Œç®—æ³•
    sha256 = hashlib.sha256()
    sha256.update(text.encode('utf-8'))
    return sha256.hexdigest()


def get_embedding(model_name):
    if embeddings.get(model_name) is None:
        embedding = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )
        embeddings[model_name] = embedding
    return embeddings[model_name]


def get_retriever(drop_old=False, k=10, collection=None):
    global milvus_retriever
    milvus_retriever = None
    embedding = get_embedding('./bge-base-zh')
    if milvus_retriever is None:
        vector_store = Milvus(
            embedding_function=embedding,
            collection_name=collection,
            connection_args={
                'host': milvus_config['host'],
                'port': milvus_config['port']
            },
            auto_id=True,
            drop_old=drop_old
        )
        milvus_retriever = vector_store.as_retriever(search_kwargs={'k': k})
    return milvus_retriever


def set_result_search(question, res1, res2):
    merged_list = []
    result_recall_unique = []
    for item in res1:
        merged_list.append(item)
    for item in res2:
        merged_list.append(item)
    # ä½¿ç”¨setå»é‡
    unique_set = set(merged_list)
    for item in unique_set:
        result_recall_unique.append({'question_text': question, 'content': item})
        # result_recall_unique.append(item)

    return result_recall_unique


def get_ranker(question, content_list, num):
    # åŠ è½½æ¨¡å‹å’Œtokenizer  BGEæœ¬åœ°æ¨¡å‹è·¯å¾„
    model_path = "./bge-reranker"
    reranker = FlagReranker(model_path, use_fp16=True)

    # å­˜å‚¨ç»“æœçš„åˆ—è¡¨
    result_list = []

    def compute_similarity(content):
        score = reranker.compute_score([question, content], normalize=True)
        return score, content

    with ThreadPoolExecutor() as executor:
        # å¹¶è¡Œè®¡ç®—ç›¸ä¼¼åº¦
        similarities = executor.map(compute_similarity, content_list)

    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œç´¢å¼•æˆªå–è·å¾—æƒ³è¦å¬å›å†…å®¹çš„ä¸ªæ•°
    top_5_similarities = sorted(similarities, key=lambda x: x[0], reverse=True)[:num]

    for score, content in top_5_similarities:
        content = str(content).strip()
        result_list.append(content)

    return result_list


def bert_encode_string(s, need_cache=False):
    if s in _bert_cache:
        return _bert_cache[s]
    global _bert_tokenizer, _bert_model
    _bert_tokenizer = None
    _bert_model = None
    if _bert_tokenizer is None:
        _bert_tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese', clean_up_tokenization_spaces=True)
    if _bert_model is None:
        _bert_model = BertModel.from_pretrained('./bert-base-chinese')
    inputs = _bert_tokenizer(s, return_tensors='pt')
    outputs = _bert_model(**inputs)
    # å–[CLS]æ ‡è®°çš„å‘é‡è¡¨ç¤º
    return outputs.last_hidden_state[:, 0, :].detach().numpy()


def bert_ranker_search(query: str, res_data, k: int = None):
    # å‘é‡åŒ–ç¼–ç å¹¶å­˜å…¥å†…å­˜
    list_data = [item['content'] for item in res_data]
    encoded_strings = {s: bert_encode_string(s[:512], True) for s in list_data}
    # æ¡ä»¶å­—ç¬¦ä¸²
    encoded_query = bert_encode_string(query, False)
    result_fine_recall_milvus = []
    # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
    similarities = {s: cosine_similarity(encoded_query, v).flatten()[0] for s, v in encoded_strings.items()}
    sorted_strings = sorted(similarities.items(), key=lambda item: item[1], reverse=True)
    for s, similarity in sorted_strings:
        content = None
        for part in res_data:
            if part['content'] in s or s in part['content']:
                content = part['content']
                break
        result_fine_recall_milvus.append({
            "question_text": query,
            "content": content
        })
        if len(result_fine_recall_milvus) >= k:
            return result_fine_recall_milvus


class CoarseRecall:

    def __init__(self, query, coarse_size, es_name, milvus_name):
        self.query = query
        self.coarse_size = coarse_size
        self.es_name = es_name
        self.milvus_name = milvus_name

    def es_recall_search(self):
        if self.query:
            s = Search(using=es_client, index=self.es_name) \
                .query('match', content=self.query) \
                .extra(size=self.coarse_size)
        else:
            s = Search(using=es_client, index=self.es_name) \
                .query(Q('function_score',
                         functions=[{'random_score': {}}],
                         boost_mode='replace')) \
                .extra(size=self.coarse_size)
        ret = s.execute()
        result_recall_es_search = []
        for doc in ret:
            text = doc.content
            # result_recall_es_search.append({
            #     "question_text": self.query,
            #     "content": text
            # })
            result_recall_es_search.append(text)

        return result_recall_es_search, ret

    def milvus_recall_search(self):
        if self.query:
            res = get_retriever(k=int(self.coarse_size), collection=self.milvus_name).invoke(self.query)
        else:
            res = get_retriever(k=int(self.coarse_size), collection=self.milvus_name).invoke('')
        result_recall_milvus_search = []
        for doc in res:
            text = doc.page_content
            result_recall_milvus_search.append(text)
        return result_recall_milvus_search, res


def recall(request):
    context = {}
    if request.method == 'GET':
        if request.user.is_authenticated:
            return render(request, 'get_data.html')
        return render(request, 'login.html', context)

    if request.method == 'POST':
        question = request.POST.get('question')
        es_name = request.POST.get('es')
        milvus_name = request.POST.get('milvus')
        coarse_size = request.POST.get('coarse_topk')
        fine_size = request.POST.get('fine_topk')

        if not question or not coarse_size or not fine_size:
            return JsonResponse({'message': 'æ³¨æ„âš ï¸å¿…å¡«é¡¹ä¿¡æ¯å­˜åœ¨é—æ¼ã€è¯·å®Œæ•´ä¿¡æ¯åæäº¤ï¼ğŸ˜¤'})
        elif not es_name and not milvus_name:
            return JsonResponse({'message': 'è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªçŸ¥è¯†åº“çš„åç§°ï¼ğŸ™„'})

        coarse_recall = CoarseRecall(
            query=question,
            coarse_size=coarse_size,
            es_name=es_name,
            milvus_name=milvus_name  # å¦‚æœä¸ä½¿ç”¨Milvusï¼Œå¯ä»¥ä¼ é€’Noneæˆ–è®¾ç½®é»˜è®¤å€¼
        )
        es_res = []
        milvus_res = []

        if es_name and not milvus_name:
            es_res, _ = coarse_recall.es_recall_search()
        elif not es_name and milvus_name:
            milvus_res, _ = coarse_recall.milvus_recall_search()
        else:
            es_res, _ = coarse_recall.es_recall_search()
            milvus_res, _ = coarse_recall.milvus_recall_search()

        set_res = set_result_search(question, es_res, milvus_res)
        bert_res = bert_ranker_search(question, set_res, int(fine_size))

        res_list = []

        for part in bert_res:
            item = part['content']
            res_list.append(item)

        return JsonResponse({'message': res_list})


# ----------------------------------------------------------------------------------------------------------------------


# ----------------------------------------------- çŸ¥è¯†å½•å…¥åŠŸèƒ½ -----------------------------------------------------------

class KnowInsert:

    def __init__(self, df, es_index_name, milvus_index_name, es_embedding, milvus_embedding):
        self.df = df
        self.es_index_name = es_index_name
        self.milvus_index_name = milvus_index_name
        self.es_embedding = es_embedding
        self.milvus_embedding = milvus_embedding

    async def es_data_ruku(self):
        try:
            if es_client.indices.exists(index=self.es_index_name):
                es_client.indices.delete(index=self.es_index_name)
            settings = {
                "analysis": {"analyzer": {"default": {"type": "standard"}}},
                "similarity": {
                    "custom_bm25": {
                        "type": self.es_embedding,
                        "k1": 2.,
                        "b": .75,
                    }
                },
            }
            mappings = {
                "properties": {
                    "content": {
                        "type": "text",
                        "similarity": "custom_bm25",  # Use the custom BM25 similarity
                    }
                }
            }
            es_client.indices.create(index=self.es_index_name, mappings=mappings, settings=settings)
        except Exception as e:
            print(f"Error creating Elasticsearch index: {e}")

        try:
            if isinstance(self.df, pd.DataFrame):
                df_renamed = self.df.rename(columns={'chunk': 'content'})
                df_list = df_renamed.to_dict('records')
                num = len(df_list)
                for row in df_list:
                    es_client.index(index=self.es_index_name, body=row)
            else:
                num = len(self.df)
                for item in self.df:
                    es_client.index(index=self.es_index_name,
                                    body={'content': item["content"], 'content_id': item["content_id"]})

            print(f'ESåº“å·²æ›´æ–°ï¼Œç´¢å¼•ä¸º{self.es_index_name}')
            index_statute = "å·²å®Œæˆ"
            es_values = (num, index_statute, self.es_index_name, 'ES')
            mysql_update(es_values)
            return num
        except Exception as eror:
            print(f"Error adding texts to Elasticsearch: {eror}")

    async def milvus_data_ruku(self):

        model_name = './' + self.milvus_embedding
        embedding = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )

        vector_store = Milvus(
            embedding_function=embedding,
            collection_name=self.milvus_index_name,
            connection_args={
                'host': milvus_config['host'],
                'port': milvus_config['port']
            },
            auto_id=True,
            drop_old=True
        )

        if isinstance(self.df, pd.DataFrame):
            texts = self.df['chunk'].tolist()
            other_fields = self.df.columns.tolist()
            other_fields.remove('chunk')
            other_df = self.df[other_fields]
            tables_info = other_df.to_dict('records')
            total = len(texts)
            vector_store.add_texts(texts=texts, metadatas=tables_info)

        else:
            total = len(self.df)
            for text in self.df:
                table_info = {"content_id": text['content_id']}
                vector_store.add_texts(texts=[text["content"]], metadatas=[table_info])

        print(f'Milvusåº“å·²æ›´æ–°ï¼Œç´¢å¼•ä¸º{self.milvus_index_name}')
        index_statute = "å·²å®Œæˆ"
        Milvus_values = (total, index_statute, self.milvus_index_name, 'Milvus')
        mysql_update(Milvus_values)

        return total


def mysql_add_userdata(values):
    cursor = conn.cursor()
    insert_stmt = (
        "INSERT INTO KNOW_USER_INFO (username,database_type,embedding,index_name,get_date,project_name,index_statute,"
        "know_content) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)"
    )
    # è¦æ’å…¥çš„æ•°æ®
    data = values

    # æ‰§è¡ŒSQLè¯­å¥
    cursor.execute(insert_stmt, data)
    conn.commit()


def mysql_update(values):
    cursor = conn.cursor()
    insert_stmt = (
        "update KNOW_USER_INFO set kg_count = %s,index_statute=%s where index_name =%s and database_type = %s"
    )
    # è¦æ’å…¥çš„æ•°æ®
    data = values

    # æ‰§è¡ŒSQLè¯­å¥
    cursor.execute(insert_stmt, data)
    conn.commit()


def handle_uploaded_file(file, file_name):
    file_path = f"./scoring/uploaded_files/{file_name}"

    with open(file_path, "wb+") as destination:
        for chunk in file.chunks():
            destination.write(chunk)

    return file_path


async def sync_function_correctly(content_list, result_list, type_name, model_name, index_name):
    if type_name == 'ES':
        know_insert_query = KnowInsert(
            df=result_list,
            es_index_name=index_name + '_query_all',
            milvus_index_name=None,
            es_embedding=model_name,
            milvus_embedding=None
        )
        await asyncio.create_task(know_insert_query.es_data_ruku())

    elif type_name == 'Milvus':
        know_insert_query = KnowInsert(
            df=result_list,
            es_index_name=None,
            milvus_index_name=index_name + '_query_all',
            es_embedding=None,
            milvus_embedding=model_name
        )
        await asyncio.create_task(know_insert_query.milvus_data_ruku())


async def sync_function_querynum(df, index_name, model_name, type_name, project_name):
    if isinstance(df, pd.DataFrame):
        know_insert = KnowInsert(
            df=df,
            es_index_name=str(project_name) + '_es',
            milvus_index_name=str(project_name) + '_milvus',
            es_embedding='BM25',
            milvus_embedding='bge-base-zh'
        )
        await asyncio.create_task(know_insert.es_data_ruku())
        await asyncio.create_task(know_insert.milvus_data_ruku())

    elif type_name == 'ES' and not project_name:
        know_insert = KnowInsert(
            df=df,
            es_index_name=index_name,
            milvus_index_name=None,
            es_embedding=model_name,
            milvus_embedding=None
        )
        await asyncio.create_task(know_insert.es_data_ruku())

    elif type_name == 'Milvus' and not project_name:
        know_insert = KnowInsert(
            df=df,
            es_index_name=None,
            milvus_index_name=index_name,
            es_embedding=None,
            milvus_embedding=model_name
        )
        await asyncio.create_task(know_insert.milvus_data_ruku())


def get_prompt(filename):
    with open(filename, 'r') as f:
        texts = [i.strip() for i in f.read().strip().replace('\r', '').split('\n\n')]
    return [{'content_id': str(uuid.uuid4()).replace("-", ""), 'content': text} for text in texts]


def upload(request):
    context = {}
    if request.method == 'GET':
        if request.user.is_authenticated:
            return render(request, 'knowledge.html', context)
        return render(request, 'login.html', context)

    elif request.method == 'POST':
        user = request.user
        up_file = request.FILES.get('file')
        up_dataframe = request.POST.get('up_dataframe')
        index_name = request.POST.get('name')
        model_name = request.POST.get('embedding')
        project_name = request.POST.get('project_name')
        know_content = request.POST.get('content')

        if isinstance(up_dataframe, pd.DataFrame):
            project_name = project_name.strip()
            asyncio.run(
                sync_function_querynum(up_dataframe, None, None, index_name, project_name))

        else:
            if not up_file or not index_name or not model_name:
                error_message = 'æ³¨æ„âš ï¸å¿…å¡«é¡¹ä¿¡æ¯å­˜åœ¨é—æ¼ã€è¯·å°†ä¿¡æ¯è¡¥å……å®Œæ•´åæäº¤ï¼ğŸ˜¤'
                return JsonResponse({'message': error_message})
            cursor = conn.cursor()
            check_stmt = "select count(1) from KNOW_USER_INFO where index_name = %s"
            cursor.execute(check_stmt, (index_name,))
            if cursor.fetchall()[0][0]:
                return JsonResponse({'message': f'é›†åˆåç§°{index_name}é‡å¤äº†ï¼Œè¯·ä¿®æ”¹ï¼'})

            type_name, model_name = model_name.split('@', 1)
            file_path = handle_uploaded_file(up_file, str(up_file))
            index_name = index_name.strip()

            texts = []
            df = pd.read_excel(file_path, engine='openpyxl')
            for index, row in df.iterrows():
                row_string = ', '.join([str(value) for value in row])
                texts.append(row_string)
            content_list = [
                {'content_id': str(uuid.uuid4()).replace("-", ""), 'content': text, 'index_name': index_name}
                for text in texts]

            now = datetime.now()
            if type_name == 'ES':
                es_values = (str(user), type_name, model_name, index_name, now, project_name, 'æ‰§è¡Œä¸­', know_content)
                # es_values_all = (
                #     str(user), type_name, model_name, index_name + '_query_all', now, project_name, 'æ‰§è¡Œä¸­',
                #     know_content)
                mysql_add_userdata(es_values)
                # mysql_add_userdata(es_values_all)
                asyncio.run(
                    sync_function_querynum(content_list, index_name, model_name, type_name, None))
            elif type_name == 'Milvus':
                miv_values = (str(user), type_name, model_name, index_name, now, project_name, 'æ‰§è¡Œä¸­', know_content)
                # miv_values_all = (
                #     str(user), type_name, model_name, index_name + '_query_all', now, project_name, 'æ‰§è¡Œä¸­',
                #     know_content)
                mysql_add_userdata(miv_values)
                # mysql_add_userdata(miv_values_all)
                asyncio.run(
                    sync_function_querynum(content_list, index_name, model_name, type_name, None))

            # pattern = r'é—®é¢˜ï¼š\s*(.*?)(?=\né—®é¢˜ï¼š|\n|$)'
            # result_list = []
            # count = 0
            # for index, item in enumerate(content_list, start=0):
            #     count += 1
            #     content = item['content'].replace('\n', '')
            #     prompt = f"""
            #         ä½ æ˜¯ä¸€ä¸ªé—®é¢˜æ”¹å†™åŠ©æ‰‹ï¼Œæ ¹æ®ä»¥ä¸‹å‚è€ƒé—®é¢˜ï¼Œæ”¹å†™æå‡º2ä¸ªæ–°çš„é—®é¢˜ï¼Œä½ éœ€æŒ‰ç…§å¦‚ä¸‹æµç¨‹å·¥ä½œ:
            #         1. æ€è€ƒç†è§£å‚è€ƒé—®é¢˜çš„å«ä¹‰ã€‚
            #         2. æ ¹æ®ä»¥ä¸Šæ€è€ƒæ”¹å†™ç”Ÿæˆæ–°é—®é¢˜ï¼Œé—®é¢˜å†…å®¹ç´§å¯†å›´ç»•å‚è€ƒé—®é¢˜ã€‚é—®é¢˜éœ€åŒ…å«æ¸…æ™°çš„ä¸»è¯­å’Œæé—®è¯‰æ±‚ã€‚
            #
            #         ## æ³¨æ„äº‹é¡¹
            #         1. å¯¹äºç”Ÿæˆçš„é—®é¢˜ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šï¼Œä»…ä½¿ç”¨ç²¾ç‚¼ã€æ¸…æ™°çš„æªè¾ã€‚
            #         2. æ ¹æ®å‚è€ƒé—®é¢˜ï¼Œç”Ÿæˆ4ä¸ªé—®é¢˜ã€‚å¹¶å°†ç”Ÿæˆçš„æ¯ä¸ªé—®é¢˜å¯¹ã€éƒ½ä»¥æ¢è¡Œçš„æ ¼å¼è¿›è¡Œè¾“å‡ºã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡º:
            #         é—®é¢˜ï¼šã€ç”Ÿæˆçš„é—®é¢˜ï¼Ÿã€‘\n
            #         é—®é¢˜ï¼šã€ç”Ÿæˆçš„é—®é¢˜ï¼Ÿã€‘
            #
            #         ## å‚è€ƒèµ„æ–™ï¼š{clean_text(content)}
            # """
            #     try:
            #         llm_result = get_Spark(prompt)
            #     except Exception as e:
            #         continue
            #     matches = re.findall(pattern, llm_result, re.DOTALL)
            #     result_list.extend([{'content': match, "content_id": item['content_id']} for match in matches])
            #
            # query = "INSERT INTO questions_answers (q_a, q_a_id, index_name) VALUES (%s, %s, %s)"
            # cursor.executemany(query, [(content["content"], content["content_id"], content["index_name"])
            #                            for content in content_list])
            # conn.commit()
            # # å¼‚æ­¥æ‰§è¡Œ
            # asyncio.run(
            #     sync_function_correctly(content_list, result_list, type_name, model_name, index_name))

        return render(request, 'knowledge.html')


# æ¨¡æ‹Ÿä¸€ä¸ª API POST è¯·æ±‚
def simulate_post_request(user, up_dataframe, project_name):
    request = HttpRequest()
    request.method = 'POST'
    request.POST = {
        'up_dataframe': up_dataframe,
        'project_name': project_name
    }
    # æ¨¡æ‹Ÿä¸€ä¸ªç”¨æˆ·
    request.user = user  # ä½ å¯ä»¥è®¾ç½®ä¸ºä¸€ä¸ªæœ‰æ•ˆçš„ç”¨æˆ·å¯¹è±¡ï¼Œå¦‚æœéœ€è¦
    return request


def select(request):
    if request.method == 'GET':
        user = request.user

        cursor = conn.cursor()
        sql = """SELECT id, database_type, index_name, index_statute FROM KNOW_USER_INFO 
        WHERE username = %s AND index_name NOT LIKE '%_query_all' ORDER BY database_type"""
        cursor.execute(sql, (str(user),))
        results = cursor.fetchall()
        list_of_lists = [[item[2], item[1], item[0], item[-1]] for item in results]

        return JsonResponse({'message': list_of_lists})


def detail(request, id):
    context = {}
    if request.method == 'GET':
        if not request.user.is_authenticated:
            return render(request, 'login.html', context)

        # åˆ›å»ºæ¸¸æ ‡å¯¹è±¡
        cursor = conn.cursor()
        # å®šä¹‰æŸ¥è¯¢ SQL

        sql = """SELECT id, database_type, index_name, kg_count, get_date, project_name, know_content 
        FROM KNOW_USER_INFO WHERE username = %s and id = %s ORDER BY database_type"""

        # æ‰§è¡ŒæŸ¥è¯¢
        user = request.user
        cursor.execute(sql, (str(user), id))
        results = cursor.fetchall()[0]

        context["ID"] = results[0]
        context["kg_type"] = results[1]
        context["kg_name"] = results[2]
        context["kg_count"] = results[3]
        context["kg_create_time"] = results[4].strftime("%Y-%m-%d %H:%M:%S")
        context["project_name"] = results[-2]
        context["know_content"] = results[-1]

        # é¢„è§ˆ10æ¡æ•°æ®
        if context["kg_type"] == "ES":
            coarse_recall = CoarseRecall(
                query=None,
                coarse_size=10,
                es_name=context["kg_name"],
                milvus_name=None  # å¦‚æœä¸ä½¿ç”¨Milvusï¼Œå¯ä»¥ä¼ é€’Noneæˆ–è®¾ç½®é»˜è®¤å€¼
            )
            Review_Recall, _ = coarse_recall.es_recall_search()
            context["kg_data_view"] = Review_Recall

        if context["kg_type"] == "Milvus":
            coarse_recall = CoarseRecall(
                query=None,
                coarse_size=10,
                es_name=None,
                milvus_name=context["kg_name"]  # å¦‚æœä¸ä½¿ç”¨Milvusï¼Œå¯ä»¥ä¼ é€’Noneæˆ–è®¾ç½®é»˜è®¤å€¼
            )
            Review_result, _ = coarse_recall.milvus_recall_search()
            Review_result = [item for item in Review_result if item != '']
            context["kg_data_view"] = Review_result

        return render(request, "detail.html", context=context)


def get_recall(request, id):
    context = {}
    if not request.user.is_authenticated:
        return render(request, 'login.html', context)

    cursor = conn.cursor()
    user = request.user
    if ',' not in id:
        sql = """SELECT id, database_type, index_name FROM KNOW_USER_INFO  WHERE username = %s and id = %s 
        ORDER BY database_type"""
        # æ‰§è¡ŒæŸ¥è¯¢
        cursor.execute(sql, (str(user), int(id)))
        results = cursor.fetchall()[0]
        if results[1] == 'ES':
            context["es_name"] = results[2]
        elif results[1] == 'Milvus':
            context["milvus_name"] = results[2]
    else:
        name = id.split(',')
        sql = """SELECT id, database_type, index_name FROM KNOW_USER_INFO  WHERE username = %s and id in (%s, %s) 
        ORDER BY database_type"""
        cursor.execute(sql, (str(user), name[0], name[-1]))
        results = cursor.fetchall()
        context["es_name"] = results[0][-1]
        context["milvus_name"] = results[1][-1]
    return render(request, "get_data.html", context=context)


def to_llm_id(request):
    if not request.user.is_authenticated:
        return render(request, 'login.html')

    cursor = conn.cursor()
    sql = ("SELECT database_type, index_name FROM KNOW_USER_INFO WHERE username = %s "
           "AND index_name NOT LIKE '%_query_all' ORDER BY database_type")
    # æ‰§è¡ŒæŸ¥è¯¢
    user = request.user
    cursor.execute(sql, (str(user),))
    results = cursor.fetchall()
    database_dict = defaultdict(list)

    # å°†æŸ¥è¯¢ç»“æœæ•´ç†åˆ°å­—å…¸ä¸­
    for db_type, index_name in results:
        database_dict[db_type].append(index_name)

    # å°† defaultdict è½¬æ¢ä¸ºæ™®é€šå­—å…¸
    final_dict = dict(database_dict)
    return render(request, "llm.html", context={'final_dict': final_dict})


def delete_know(request, id):
    if not request.user.is_authenticated:
        return render(request, 'login.html')

    cursor = conn.cursor()
    query_stmt = "select index_name,database_type,embedding from KNOW_USER_INFO where id=%s"
    # æ‰§è¡ŒSQLè¯­å¥
    cursor.execute(query_stmt, (id,))
    kg_config = cursor.fetchall()[0]
    index_name = kg_config[0]
    database_type = kg_config[1]

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²ç»å­˜åœ¨
    if database_type == "ES":
        if es_client.indices.exists(index=index_name):
            es_client.indices.delete(index=index_name)
            # es_client.indices.delete(index=index_name + '_query_all')
    if database_type == "Milvus":
        client = MilvusClient(uri=f"http://{milvus_config['host']}:{milvus_config['port']}")
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨äºMilvusä¸­
        try:
            # åˆ é™¤é›†åˆ
            client.drop_collection(index_name)
            # client.drop_collection(index_name + '_query_all')
            print(f"Collection '{index_name}' has been deleted successfully.")
        except Exception as e:
            print(f"{e}\nCollection '{index_name}' does not exist.")

    cursor = conn.cursor()
    del_stmt = "delete from KNOW_USER_INFO where id = %s"
    # è¦æ’å…¥çš„æ•°æ® user, database_type, index_name, embedding, now,num
    # æ‰§è¡ŒSQLè¯­å¥
    cursor.execute(del_stmt, (id,))
    conn.commit()
    return render(request, 'knowledge.html')


def download(request):
    if not request.user.is_authenticated:
        return render(request, 'login.html')
    # æ„å»ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    file_path = os.path.join(BASE_DIR, 'scoring', 'templates', 'upload_template.xlsx')

    if not os.path.exists(file_path):
        raise Http404("File does not exist")

    return FileResponse(open(file_path, 'rb'), as_attachment=True, filename='upload_template.xlsx')


def get_content(ids):
    if not ids:
        return []
    cursor = conn.cursor()
    # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ä»¥é˜²æ­¢SQLæ³¨å…¥
    query = "SELECT q_a FROM questions_answers WHERE q_a_id IN (%s)" % ','.join(['%s'] * len(ids))
    cursor.execute(query, ids)
    results = cursor.fetchall()
    cursor.close()
    return [row[0] for row in results]


def llm_chat(request):
    if not request.user.is_authenticated:
        return render(request, 'login.html')

    if request.method == 'POST':
        try:
            body_str = request.body.decode('utf-8')
            data = json.loads(body_str)
            scene = data['scene']
            es_index_name = data.get('esKnowledge')
            milvus_index_name = data.get('milvusKnowledge')
            llm_type = data.get('llmModel')
            cu_coarse_size = data.get('coarseRecall')
            jg_coarse_size = data.get('fineRecall')
            query_count = data.get('rewriteCount')
            query = data.get('message')
            temperature = data.get('temperature')

            if not scene or not es_index_name or not milvus_index_name or not llm_type:
                def check():
                    yield f"data: {json.dumps({'message': 'è¯·å°†å¿…å¡«ä¿¡æ¯è¡¥å……å®Œæ•´å†è¿›è¡Œæé—®ï¼ğŸ˜¤'})}\n\n"

                return StreamingHttpResponse(check(), content_type='application/json')

            matches = [query]
            if query_count:
                query_prompt = f"""
                    ä½ æ˜¯ä¸€ä¸ªé—®é¢˜æ”¹å†™åŠ©æ‰‹ï¼Œæ ¹æ®ä»¥ä¸‹å‚è€ƒé—®é¢˜ï¼Œæ”¹å†™æå‡º{query_count}ä¸ªæ–°çš„é—®é¢˜ï¼Œä½ éœ€æŒ‰ç…§å¦‚ä¸‹æµç¨‹å·¥ä½œ:
                    1. æ€è€ƒç†è§£å‚è€ƒé—®é¢˜çš„å«ä¹‰ã€‚
                    2. æ ¹æ®ä»¥ä¸Šæ€è€ƒæ”¹å†™ç”Ÿæˆæ–°é—®é¢˜ï¼Œé—®é¢˜å†…å®¹ç´§å¯†å›´ç»•å‚è€ƒé—®é¢˜ã€‚é—®é¢˜éœ€åŒ…å«æ¸…æ™°çš„ä¸»è¯­å’Œæé—®è¯‰æ±‚ã€‚

                    ## æ³¨æ„äº‹é¡¹
                    1. å¯¹äºç”Ÿæˆçš„é—®é¢˜ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šï¼Œä»…ä½¿ç”¨ç²¾ç‚¼ã€æ¸…æ™°çš„æªè¾ã€‚
                    2. æ ¹æ®å‚è€ƒé—®é¢˜ï¼Œç”Ÿæˆ{query_count}ä¸ªé—®é¢˜ã€‚å¹¶å°†ç”Ÿæˆçš„æ¯ä¸ªé—®é¢˜å¯¹ã€éƒ½ä»¥æ¢è¡Œçš„æ ¼å¼è¿›è¡Œè¾“å‡ºã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡º:
                    é—®é¢˜ï¼šã€ç”Ÿæˆçš„é—®é¢˜ï¼Ÿã€‘\n
                    é—®é¢˜ï¼šã€ç”Ÿæˆçš„é—®é¢˜ï¼Ÿã€‘

                    ## å‚è€ƒé—®é¢˜ï¼š{query}
                """
                query_llm = None
                if llm_type == 'æ–‡å¿ƒåƒå¸†':
                    query_llm = get_Qianfan(query_prompt)
                elif llm_type == 'é€šä¹‰åƒé—®':
                    query_llm = get_Tongyi(query_prompt)
                elif llm_type == 'è®¯é£æ˜Ÿç«':
                    query_llm = get_Spark(query_prompt)
                pattern = r'é—®é¢˜ï¼š(.+?)\ï¼Ÿ'
                match_pat = re.findall(pattern, query_llm)
                if match_pat:
                    matches.extend(match_pat)

            ES_Review_result = None
            Milvus_Review_result = None
            ranker_list = []
            for match in matches:
                coarse_recall = CoarseRecall(
                    query=match,
                    coarse_size=cu_coarse_size,
                    es_name=es_index_name,
                    milvus_name=milvus_index_name
                )
                ES_Review_result, _ = coarse_recall.es_recall_search()
                Milvus_Review_result, _ = coarse_recall.milvus_recall_search()
                # é’ˆå¯¹é€šè¿‡LLMç”Ÿæˆé—®é¢˜çš„çŸ¥è¯†åº“ä½¿ç”¨çš„ä»£ç 
                # coarse_recall_all = CoarseRecall(
                #     query=match,
                #     coarse_size=cu_coarse_size,
                #     es_name=es_index_name + '_query_all',
                #     milvus_name=milvus_index_name + '_query_all'
                # )
                # _, ES_Review_query = coarse_recall_all.es_recall_search()
                # es_ids = [content['content_id'] for content in ES_Review_query]
                # es_list = get_content(es_ids)
                # _, Milvus_Review_query = coarse_recall_all.milvus_recall_search()
                # milvus_ids_q = [doc.metadata["content_id"] for doc in Milvus_Review_query]
                # milvus_list = get_content(milvus_ids_q)
                # set_res = set_result_search(query, ES_Review_result + es_list, Milvus_Review_result +
                #                             milvus_list)
                set_res = set_result_search(query, ES_Review_result, Milvus_Review_result)
                ranker_list += set_res

            if scene == 'é€šç”¨åœºæ™¯':
                ranker_result = bert_ranker_search(query, ranker_list, int(jg_coarse_size))
                res_list = []
                for part in ranker_result:
                    item = part['content']
                    if part['content'] not in res_list:
                        res_list.append(item)

                prompt_text = '\n\n'.join(res_list)
                llm_prompt = f"ä½ æ˜¯ä¸€åæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œç»“åˆå·²çŸ¥ä¿¡æ¯è¿›è¡Œæ€è€ƒå¹¶è§£ç­”ã€‚\n\n" \
                             f"## çŸ¥è¯†åº“å·²çŸ¥ä¿¡æ¯ï¼š\n {prompt_text} \n\n## æå‡ºçš„é—®é¢˜ï¼š{query}\n\n### æ€è€ƒè¿‡ç¨‹ï¼š\nã€æ­¤å¤„è¯·å›ç­”ä½ ç»™å‡ºè¯¥è§£ç­”çš„æ€è€ƒè¿‡ç¨‹ã€‘" \
                             f"\n### å›ç­”ï¼š\nã€æ­¤å¤„è¯·ä¾æ®å·²çŸ¥å†…å®¹ä¿¡æ¯è¿›è¡Œè§£ç­”ã€‘"

            elif scene == 'è¥é”€å®¡è®¡':
                ES_top_data = []
                Milvus_top_data = []
                pattern = r'ã€(.*?)ã€‘'
                for i, es_doc in enumerate(ES_Review_result, start=1):
                    es_recall = es_doc.replace('æ³•è§„ï¼š', '').replace('\n', '').strip()
                    es_statute = es_recall.split('æ¡æ–‡ï¼š')[0]
                    es_clause = es_recall.split('æ¡æ–‡ï¼š')[1]
                    if es_statute and es_clause:
                        ES_top_data.append(f'ã€æ³•è§„åç§°:{es_statute}ã€‘--ç›¸å…³æ¡æ¬¾:{es_clause}')

                for i, miv_doc in enumerate(Milvus_Review_result, start=1):
                    miv_recall = miv_doc.replace('æ³•è§„ï¼š', '').replace('\n', '').strip()
                    miv_statute = miv_recall.split('æ¡æ–‡ï¼š')[0]
                    miv_clause = miv_recall.split('æ¡æ–‡ï¼š')[1]
                    if miv_statute and miv_clause:
                        Milvus_top_data.append(f'ã€æ³•è§„åç§°:{miv_statute}ã€‘--ç›¸å…³æ¡æ¬¾:{miv_clause}')

                combined_list = ES_top_data + Milvus_top_data
                distinct_list = list(OrderedDict.fromkeys(combined_list))
                content_list = []
                ranker_top_data = []
                ranker_recall = {}
                # ç²—å¬å› TOP ç»“æœä¸¢ç»™ ranker
                for i, result in enumerate(distinct_list):
                    match = re.search(pattern, result)
                    if match:
                        content_in = result.split('--')[1]
                        session_in = match.group(1)
                        content_list.append(content_in)
                        ranker_recall[generate_unique_identifier(content_in.strip())] = session_in
                ranker_result = get_ranker(query, content_list, int(jg_coarse_size))

                for content in ranker_result:
                    session = ranker_recall[generate_unique_identifier(content.strip())].strip('--')
                    new_content = content.replace('æ³•è§„åç§°', '### æ³•è§„åç§°').replace('ç›¸å…³æ¡æ¬¾', '\n#### ç›¸å…³æ¡æ¬¾')
                    ranker_top_data.append(f'ã€{session.strip()}ã€‘--{new_content.strip()}')

                result_str = ""
                data_dict = defaultdict(list)
                for qa in ranker_top_data:
                    filtered = qa.split('ã€‘--')[0].strip('ã€')
                    cnt = qa.split('ã€‘--')[1]
                    if filtered:
                        data_dict[filtered].append(cnt)
                    else:
                        result_str += f"\n{cnt}\n"

                for filtered, cnt_list in data_dict.items():
                    result_str += f"\n{filtered}\n"
                    for cnt in cnt_list:
                        result_str += f"{cnt}\n"

                llm_prompt = f"ä½ æ˜¯ä¸€ä¸ªè¥é”€å®¡è®¡ä¸“å®¶ï¼Œå…·å¤‡ä¸“ä¸šçš„è¥é”€å®¡è®¡æ³•å¾‹çŸ¥è¯†ï¼Œå¯é’ˆå¯¹ç”¨æˆ·æå‡ºçš„é—®é¢˜è¿›è¡Œè§£ç­”ã€å¹¶æ¨èå‡ºç›¸åº”çš„æ³•è§„ã€æ¡æ¬¾ã€‚" \
                             f"\n\n## å·²çŸ¥è¥é”€å®¡è®¡æ³•è§„ã€ç›¸å…³æ¡æ¬¾ï¼š\n{result_str}\nè¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œè®¤çœŸé˜…è¯»æ¯éƒ¨æ³•è§„åŠç›¸å…³æ¡æ¬¾ï¼Œå¹¶æ€è€ƒæ¯ä¸ªæ¡æ¬¾é€‚ç”¨äºå“ªäº›åœºæ™¯ï¼Œ" \
                             f"å¯¹ä¸‹é¢ç”¨æˆ·æå‡ºçš„é—®é¢˜è¿›è¡Œè§£ç­”å¹¶æ¨èç›¸åº”çš„æ³•è§„ã€æ¡æ¬¾ã€‚åœ¨æ¨èçš„è¿‡ç¨‹ä¸­ï¼Œè¯·ä¸€æ­¥æ­¥æ€è€ƒä¸ºä»€ä¹ˆè¿™æ ·æ¨èï¼Œå¹¶ç»™å‡ºåˆç†çš„å››ç§ä¸åŒæ³•è§„åŠç›¸å…³çš„å¤šä¸ªæ¡æ¬¾å…¨éƒ¨å†…å®¹ä¿¡æ¯" \
                             f"\n\n## é—®é¢˜ï¼š{query}\n### è§£ç­”ï¼š\nã€æ­¤å¤„è¯·æ ¹æ®å·²çŸ¥è¥é”€å®¡è®¡æ³•è§„ã€ç›¸å…³æ¡æ¬¾ä¿¡æ¯ä¸æ€è€ƒå†…å®¹å¯¹é—®é¢˜è¿›è¡Œè§£ç­”ã€‘\n" \
                             f"### ç›¸å…³æ³•è§„åŠå¯¹åº”æ¡æ¬¾ä¾æ®ï¼š\nã€æ­¤å¤„è¯·å‚è€ƒå·²çŸ¥è¥é”€å®¡è®¡æ³•è§„ã€ç›¸å…³æ¡æ¬¾ï¼Œé€‰æ‹©æ¨èå‡ºä¸é—®é¢˜æ‰€ä¾æ®çš„å››ç§ä¸åŒæ³•è§„åç§°åŠå¯¹åº”çš„å¤šä¸ªæ¡æ¬¾å®Œæ•´å†…å®¹ä¿¡æ¯å¹¶ä»¥æ¢è¡Œçš„æ–¹å¼è¿›è¡Œå›ç­”ï¼Œ" \
                             f"æ¯ä¸ªæ¨èæ³•è§„ä¸­å¯¹åº”çš„å¤šä¸ªæ¡æ¬¾å®Œæ•´å†…å®¹ä¿¡æ¯ä»¥æ— åºåˆ—è¡¨çš„æ–¹å¼è¿›è¡Œå›ç­”ï¼ˆéœ€è¦å›ç­”å‡ºç¬¬å‡ ç« ç¬¬å‡ æ¡è§„å®šå’Œå®Œæ•´çš„æ¡ç›®å†…å®¹ï¼‰ã€‚" \
                             f"\n## æ³¨æ„ï¼šè‹¥æ¨èçš„æ³•è§„é‡åï¼Œåˆ™å°†é‡åæ³•è§„ä¸­çš„æ‰€æœ‰æ¡æ¬¾å…¨éƒ¨å½’é›†åˆ°ä¸€ä¸ªæ³•è§„ä¸‹è¾“å‡ºã€‚" \
                             f"å›ç­”çš„æ¡æ¬¾å¿…é¡»æ˜¯å·²çŸ¥ç¤ºä¾‹ä¸­å­˜åœ¨çš„ä¸”å†…å®¹å®Œæ•´ä¸å¾—ä¸¢å¤±ï¼Œç¦æ­¢å¯¹å·²çŸ¥ç¤ºä¾‹ä¸­æ¡æ¬¾è¿›è¡Œæ”¹å†™æˆ–æ€»ç»“å†è¿”å›ï¼Œç¦æ­¢è¿›è¡Œæ€»ç»“æˆ–æ¨èç†ç”±ç­‰å†—ä½™ä¿¡æ¯ã€‚ã€‘" \
                             f"\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šæ ‡é¢˜çº§åˆ«æ ¼å¼è¿›è¡Œå›ç­”ï¼Œå…ˆè§£ç­”ç”¨æˆ·çš„é—®é¢˜å†æ¨èå‡ºåˆç†çš„å››ç§ä¸åŒæ³•è§„åç§°åŠæ¯éƒ¨æ³•è§„æ‰€å¯¹åº”çš„å¤šä¸ªç›¸å…³æ¡æ¬¾ï¼"

            elif scene == 'åº•ç¨¿æ¨è':
                pass

            def generate():
                full_response = ""
                if llm_type == 'qwen2:7b':
                    for char in generate_text(llm_prompt):
                        full_response += char
                        yield f"data: {json.dumps({'message': full_response})}\n\n"
                elif llm_type == 'è®¯é£æ˜Ÿç«':
                    for char in get_Spark(llm_prompt):
                        full_response += char
                        yield f"data: {json.dumps({'message': full_response})}\n\n"
                else:
                    for chunk in llm_flow(llm_prompt, float(temperature), stream_out=True, llm_type=llm_type):
                        full_response += chunk
                        # æ¯æ¬¡éƒ½å‘é€å®Œæ•´çš„å“åº”
                        yield f"data: {json.dumps({'message': full_response})}\n\n"

            return StreamingHttpResponse(generate(), content_type='application/json')
        except json.JSONDecodeError:
            return JsonResponse({'status': 'error', 'message': 'Invalid JSON'}, status=400)



def get_llm(request):
    user = request.user
    if not user.is_authenticated:
        return render(request, 'login.html')

    body_str = request.body.decode('utf-8')
    data = json.loads(body_str)
    id = data['id']
    query = data['query']

    cursor = conn.cursor()
    Review_result = None

    if ',' not in id:
        query_stmt = "select index_name,database_type,embedding from KNOW_USER_INFO where username = %s and id=%s"

        # æ‰§è¡ŒSQLè¯­å¥
        cursor.execute(query_stmt, (str(user), int(id)))
        kg_config = cursor.fetchall()[0]

        index_name = kg_config[0]
        database_type = kg_config[1]

        if database_type == "ES":
            coarse_recall = CoarseRecall(
                query=query,
                coarse_size=10,
                es_name=index_name,
                milvus_name=None
            )
            Review_result = coarse_recall.es_recall_search()

        if database_type == "Milvus":
            coarse_recall = CoarseRecall(
                query=query,
                coarse_size=10,
                es_name=None,
                milvus_name=index_name
            )
            Review_result = coarse_recall.milvus_recall_search()
        prompt_text = '\n\n'.join(Review_result)

    else:
        name = id.split(',')
        query_stmt = ("select index_name,database_type,embedding from KNOW_USER_INFO where username = %s and "
                      "id in (%s, %s) AND index_name NOT LIKE '%_query_all' ORDER BY database_type")

        cursor.execute(query_stmt, (str(user), name[0], name[-1]))
        results = cursor.fetchall()
        es_index_name = results[0][0]
        milvus_index_name = results[-1][0]
        coarse_recall = CoarseRecall(
            query=query,
            coarse_size=10,
            es_name=es_index_name,
            milvus_name=milvus_index_name
        )
        ES_Review_result, _ = coarse_recall.es_recall_search()
        Milvus_Review_result, _ = coarse_recall.milvus_recall_search()
        prompt_text = '\n\n'.join(ES_Review_result + Milvus_Review_result)

    # æ„å»ºæœ€ç»ˆçš„ prompt
    prompt = f"æ‚¨æ˜¯ä¸€åæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œç»“åˆå¹¶æ ¹æ®å·²çŸ¥ä¿¡æ¯è¿›è¡Œæ€è€ƒä¸è§£ç­”ã€‚\n\n" \
             f"## å·²çŸ¥ä¿¡æ¯ï¼š\n {prompt_text} \n\n## æå‡ºçš„é—®é¢˜ï¼š{query}\n\n### æ€è€ƒè¿‡ç¨‹ï¼š\nã€æ­¤å¤„è¯·å›ç­”ä½ ç»™å‡ºè¯¥è§£ç­”çš„æ€è€ƒè¿‡ç¨‹ã€‘" \
             f"\n### è§£ç­”ï¼šã€æ­¤å¤„è¯·ä¾æ®å·²çŸ¥å†…å®¹ä¿¡æ¯è¿›è¡Œè§£ç­”ã€‘"
    llm_one_word = get_Qianfan(prompt)
    success_message = f"é’ˆå¯¹æ‚¨çš„æé—® ä¸ºæ‚¨å›ç­”ï¼š\n\n + {llm_one_word}"
    return JsonResponse({'message': success_message})
# ----------------------------------------------------------------------------------------------------------------------
